
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <style>
    img {
      width: 100%;
      max-width: 500px;
      height: auto;
    }
  </style>
</head>

<div id="page" class="post">
  <h1 class="title">Sparsity and Graphs</h1>


  <div class="post-date">
    <time datetime="2022-03-28T00:00:00Z">Mar 28, 2022</time> <span class="readtime">&middot; 9 min read</span>
  </div>
  <div>
   <p>Graphs allow us to encode structural assumptions about data.
Graphs are the natural language for describing all kinds of problems and data.
The graphs discussed in this post will quantify statistical relationships between random variables.
In particular, there is a graph associated with any multivariate distribution.
There is always a graph associated with random variable $\omega$.</p>
<p>There are two types of graphs: Directed versus Undirected graphs.</p>
<p><img src="../../images/sds-365/graph.png" alt="Example"></p>
<p>A graph $G=(V,E)$ has vertices $V$ and edges $E$.
If $X=(X_1,.., X_p)$ is a random variable, we will study graphs with $p$ vertices, one for each $X_j$.
The graph will encode conditional independence relations among the variables.
Directed graphical models are central for causal inference.</p>
<h1 id="example">Example</h1>
<p>We have a three-dimensional Gaussian $(X,Y,Z)$ with covariance:</p>
<p>$$
\Sigma = \begin{pmatrix}
1.3 &amp; -2 &amp; 2.1\\
-2 &amp; 2.6 &amp; -2.4 \\
2.1 &amp; -2.4 &amp; 3.1
\end{pmatrix}
$$</p>
<p>So, all pairs are correlated. For example, $X$ and $Y$ have a negative correlation.</p>
<h2 id="how-to-sample">How to sample</h2>
<p>Now we know how to sample from this distribution. We factor it; we take the squared root of that matrix (i.e., Cholesky decomposition), generate standard normal random variables, and then scale by that decomposition. That gives us a sample from that Gaussian:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="hl"><span class="lnt">30
</span></span><span class="hl"><span class="lnt">31
</span></span><span class="hl"><span class="lnt">32
</span></span><span class="hl"><span class="lnt">33
</span></span><span class="hl"><span class="lnt">34
</span></span><span class="hl"><span class="lnt">35
</span></span><span class="hl"><span class="lnt">36
</span></span><span class="hl"><span class="lnt">37
</span></span><span class="hl"><span class="lnt">38
</span></span><span class="hl"><span class="lnt">39
</span></span><span class="hl"><span class="lnt">40
</span></span><span class="hl"><span class="lnt">41
</span></span><span class="hl"><span class="lnt">42
</span></span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line hl"><span class="cl">  <span class="n">d</span> <span class="o">=</span> <span class="mi">2</span>
</span></span><span class="line hl"><span class="cl">  <span class="n">m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line hl"><span class="cl">  <span class="c1"># Set covariance function.</span>
</span></span><span class="line hl"><span class="cl">  <span class="n">K_0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
</span></span><span class="line hl"><span class="cl">                <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
</span></span><span class="line hl"><span class="cl">  <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.0001</span>
</span></span><span class="line hl"><span class="cl">  <span class="c1"># Add small pertturbation. </span>
</span></span><span class="line hl"><span class="cl">  <span class="n">K</span> <span class="o">=</span> <span class="n">K_0</span> <span class="o">+</span> <span class="n">epsilon</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
</span></span><span class="line hl"><span class="cl">  <span class="n">L</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">Cholesky</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
</span></span><span class="line hl"><span class="cl">  <span class="c1"># Number of samples. </span>
</span></span><span class="line hl"><span class="cl">  <span class="n">n</span> <span class="o">=</span> <span class="mi">10000</span>
</span></span><span class="line hl"><span class="cl">  <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">d</span><span class="o">*</span><span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
</span></span><span class="line hl"><span class="cl">  <span class="n">x</span> <span class="o">=</span> <span class="n">m</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">u</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div>
<p>However, the marginal is still a normal distribution when we condition one random variable.
<img src="https://dadashkarimi.github.io/images/SDS-365/graph_condition.png" alt="Example">
which is not tilted in any direction.</p>
<p>This is revealed in the &ldquo;precision matrix&rdquo;:</p>
<p>$$
\Omega = \Sigma^{-1} =  \begin{pmatrix}
1 &amp; 0.9 &amp; 0\\
0.9 &amp; 2 &amp; 0.9 \\
0 &amp; 0.9 &amp; 1
\end{pmatrix}
$$</p>
<p>The zeros lead to <strong>conditional</strong> independence assumptions.
These <strong>conditional</strong> independence assumptions were hidden in the covariance matrix.</p>
<h2 id="undirected-graphs">Undirected graphs</h2>
<p><img src="../../images/sds-365/cond_ind_graph.png" alt="Example"></p>
<p>Here, $V={X,Y,Z}$ and $E={(X,Y),(Y,Z)}$.
This encodes the independence relation:
$$
X \perp \!\!\! \perp Z  ~  | ~  Y
$$
This means that $X$ and $Z$ are independent conditioned on $Y$.</p>
<p>Here is another example.</p>
<p><img src="../../images/sds-365/cond_ind_graph_4.png" alt="Example"></p>
<p>$$
\begin{align}
X_1 \perp \!\!\! \perp X_4  ~  | ~  X_2,X_3 \\
X_2 \perp \!\!\! \perp X_3  ~  | ~  X_1,X_4
\end{align}
$$</p>
<p>In the following graph, $C=\{3,7\}$ separates $A=\{1,2\}$ and $B=\{4,8\}$. Therefore:</p>
<p>$$
\begin{align}
\{X_1,X_2\} \perp \!\!\! \perp \{X_4,X_8\}  ~  | ~  \{X_3,X_7 \}
\end{align}
$$</p>
<p><img src="../../images/sds-365/cond_ind_graph_8.png" alt="Example"></p>
<h2 id="special-case">Special case</h2>
<p>If $(i,j) \not\in E$ then:
$$
X_i \perp \!\!\! \perp X_j | \{X_k: k \neq i,j\}
$$</p>
<p>lack of an edge from $i$ to $j$ implies that $X_i$ and $X_j$ are independent given all of the other random variables.</p>
<h2 id="graph-estimation">Graph Estimation</h2>
<p>A graph $G$ represents the class of distributions, $P(G)$, the
distributions that are Markov concerning $G$.</p>
<p>Graph estimation: Given n samples $X_1, .., X_n \sim P$, estimate the
graph $G$.</p>
<h2 id="gaussian-case">Gaussian case</h2>
<p>Let $\Omega = \Sigma^{-1}$ be the precision matrix.
A zero in $\Omega$ indicates a lack of the corresponding edge in the graph.</p>
<p>For example:</p>
<p>$$
\Omega = \Sigma^{-1} =  \begin{pmatrix}
* &amp; * &amp; 0\\
* &amp; * &amp; * \\
0 &amp; * &amp; *
\end{pmatrix}
$$</p>
<p><img src="../../images/sds-365/cond_ind_graph.png" alt="Example"></p>
<p>And when we have four random variables, we can similarly read off the graph:
$$
\Omega = \Sigma^{-1} =  \begin{pmatrix}
* &amp; * &amp; * &amp; 0\\
* &amp; * &amp; 0 &amp; *\\
* &amp; 0 &amp; * &amp; *\\
0 &amp; * &amp; * &amp; *
\end{pmatrix}
$$</p>
<p><img src="../../images/sds-365/cond_ind_graph_4.png" alt="Example"></p>
<p>$$
\begin{align}
X_1 \perp \!\!\! \perp X_4  ~  | ~  X_2,X_3
\end{align}
$$</p>
<h2 id="the-machine-learning-problem">The machine learning problem</h2>
<p>How do we estimate the graph from a sample of data?</p>
<h3 id="gaussian-case-algorithms">Gaussian case: Algorithms</h3>
<p>Two approaches:</p>
<ol>
<li>parallel lasso</li>
<li>graphical lasso</li>
</ol>
<p>In <strong>parallel</strong> lasso:</p>
<ol>
<li>For each $j = 1, .., p$ (in parallel): Regress $X_j$ on all other
variables using the lasso.</li>
<li>Put an edge between $X_i$ and $X_j$ if each appears in the regression
of the other.</li>
</ol>
<p>In <strong>graphical</strong> lasso:</p>
<ol>
<li>Assume a multivariate Gaussian model</li>
<li>Subtract out the sample mean</li>
<li>Minimize the negative log-likelihood of the data, subject to a constraint on the sum of the absolute values of the inverse covariance.</li>
</ol>
<p>The glasso optimizes the parameters of $\Omega = \Sigma^{-1}$ by minimizing:
$$
\text{trace}(\Omega S_n) - \log |\Omega| + \lambda \sum_{j\neq k}|\Omega_{jk}|
$$</p>
<p>where $|\Omega|$ is the determinant and $S_n$ is the sample covariance:</p>
<p>$$
S_n = \frac{1}{n} \sum_{i=1}^n x_ix_i^T
$$</p>
<p>If we assume the mean equals $0$, then the probability density at a data point $x$ is:</p>
<p>$$
\begin{aligned}
p(x) = &amp; \frac{1}{\sqrt{(2\pi)^p|\Sigma|}} \exp \Big( -\frac{1}{2} X^T \Sigma^{-1}X\Big) \\
=&amp; \frac{1}{\sqrt{(2\pi)^p|\Sigma|}} \exp \Big( -\frac{1}{2} X^T \Omega X\Big) \\
= &amp; \frac{1}{\sqrt{(2\pi)^p|\Sigma|}} \exp \Big( -\frac{1}{2} \text{trace}(\Omega XX^T)\Big) \\
\end{aligned}
$$</p>
<p>Therefore, using $\log |A|  = -\log |A^{-1}|$, up to an additive constant:
$$
-\log p(x) = \frac{1}{2} \log |\Sigma| + \frac{1}{2} \text{trace}(\Omega XX^T) = -\frac{1}{2} \log |\Omega|+ \frac{1}{2} \text{trace}(\Omega XX^T)
$$</p>
<p>Summing over all the data we have:</p>
<p>$$
\begin{aligned}
-\sum_{i=1}^n \log p(x_i) =&amp;  \frac{1}{2} \sum_{i=1}^n \text{trace}(\Omega X_iX_i^T) -\frac{n}{2} \log |\Omega| \\
= &amp; \frac{n}{2} \text{trace} (\Omega S_n) - \frac{n}{2} \log |\Omega|
\end{aligned}
$$</p>
<p>Rescaling by $2/n$ and adding the $\mathcal{L_1}$ penalty, we get the objective function:
$$
\mathcal{O}(\Omega) = \text{trace} (\Omega S_n) - \log |\Omega|+ \lambda \sum_{k\neq j} |\Omega_{jk}|
$$</p>
<p>This is a convex function of $\Omega$.</p>
<h2 id="discrete-graphical-models">Discrete Graphical Models</h2>
<p>There are some challenges in handling discrete data:</p>
<ol>
<li>Models don&rsquo;t have closed forms.</li>
<li>We need to use Gibbs sampling, variational inference</li>
<li>No analog of the graphical lasso</li>
</ol>
<p>Lets say $G=(V,E)$ be an undirected Graph on $m=|V|$ vertices:</p>
<p>Recall: <strong>edge</strong> could be a clique, a <strong>triangle</strong> is a clique, but a square is not a clique (i.e., we need cross edges).</p>
<p><img src="../../images/sds-365/discrete_gm.png" alt="Example"></p>
<p>A positive distribution over random variables $Z_1,.., Z_p$ that satisfies the Markov properties of graph $G$ can be represented as:</p>
<p>$$
p(Z) \propto \prod_{c\in \mathcal{C}} \psi_c(Z_c)
$$</p>
<p>It&rsquo;s a product of factors, one for each clique in the graph where $\mathcal{C}$ is the set of clubs in the graph $G$.
This is equivalent to our previous statement that a graph corresponds to a set of conditional independence assumptions.</p>
<p>Recall: Positive distributions can be represented by an exponential family:</p>
<p>$$
p(Z;\beta) \propto \exp\Big( \sum_{c\in \mathcal{C}} \beta_c \phi_c(Z_c)\Big)
$$</p>
<p>Particular case: Ising Model (discrete Gaussian):</p>
<p>$$
p(Z;\beta) \propto \exp\Big( \sum_{i\in V} \beta_i Z_i +\sum_{(i,j)\in E} \beta_{ij}Z_i Z_j \Big)
$$</p>
<p>Note that we can write a multivariate Gaussian as follows:
$$
p(z) \propto \exp\Big( \sum_{i\in V} \beta_i z_i +\sum_{(i,j)\in E} \beta_{ij} z_i z_j\Big)
$$</p>
<p>If I change the sample space from natural numbers to binary, I go from a Gaussian to an Ising model.
Another way to think about this is in terms of sufficient statistics, the first term corresponds to $x$, and the second term corresponds to $x^2$.</p>
<p>can you see what $\beta_i$ and $\beta_{ij}$ are?</p>
<h3 id="from-edges-to-cliques">From edges to cliques</h3>
<p>Take $\beta_j=0$ for simplicity. If we have a triangle $(i,j,k)$ in the graph, then the potential function corresponds to:</p>
<p>$$
\psi_{ijk}(Z_i,Z_j,Z_k) = e^{\beta_{ij} Z_i Z_j } e^{\beta_{jk} Z_j Z_k} e^{\beta_{ik} Z_i Z_k }
$$</p>
<p>This is a prevalent model in Immunology, like the risk of an outbreak.</p>
<h3 id="recall-from-a-few-weeks-ago">Recall from a few weeks ago</h3>
<p>$$
P_{\beta}(Z_1,..,Z_n) \propto \exp\Big( \sum_{s\in V}\beta_s Z_s+ \sum_{(s,t)\in E}\beta_{st}Z_sZ_t \Big)
$$</p>
<p>where $Z_i = \{0,1\}$.</p>
<p>We discussed that we couldn&rsquo;t compute this probability because of the normalizing constant. Here are some techniques to compute these:</p>
<h4 id="gibbs-sampler--stochastic-approximation">Gibbs sampler  (stochastic approximation)</h4>
<ol>
<li>Choose vertex $s\in V$ at random</li>
<li>Sample $u\sim \text{Uniform}(0,1)$ and update:</li>
</ol>
<p>$$
Z_s =
\begin{cases}
1 &amp; u \leq (1+\exp(-\beta_s-\sum_{t\in N(s)}\beta_{st}z_t))^{-1}\\
0 &amp; \text{ otherwise }
\end{cases}
$$</p>
<p>where $\beta_{st}$ denotes strength of influence node $s$ has on node $t$.
This is precisely like Logistic regression.</p>
<ol start="3">
<li>Iterate</li>
</ol>
<h4 id="mean-field-variational-algorithm-deterministic-approximation">Mean field variational algorithm (deterministic approximation)</h4>
<ol>
<li>Choose vertex $s\in V$ at random</li>
<li>Update:</li>
</ol>
<p>$$
\mu_s =
(1+\exp(-\beta_s-\sum_{t\in N(s)}\beta_{st}z_t))^{-1}
$$
3. Iterate</p>
<h2 id="graph-estimation-1">Graph Estimation</h2>
<p>The problem is this:  Given n i.i.d. samples from an Ising distribution, ${Z_i , i = 1, .. ,n}$
(each is a p-vector of {0, 1} values) identify underlying graph.
Then, we have some data points like the following graphs:</p>
<p><img src="https://dadashkarimi.github.io/images/SDS-365/graph_estimation_examples.png" alt="Example"></p>
<p>We want to go from the data points to estimate that graph.</p>
<p><img src="../../images/sds-365/graph_estimation_examples_2.png" alt="Example"></p>
<p>Consider the Ising model:
$$
\psi_{ijk}(Z_i,Z_j,Z_k) = e^{\beta_{ij} Z_i Z_j } e^{\beta_{jk} Z_j Z_k} e^{\beta_{ik} Z_i Z_k }
$$</p>
<p>Conditioned on $(z_2,.., z_p$), variable $Z_1 \in \{0,1\}$ has probability
a logistic function gives the mass function:</p>
<p>$$
P(Z_1=1 | z_2,..,z_p) = \text{sigmoid} \Big( \beta_1+ \sum_{j\in N(1)} \beta_{1,j} z_j\Big)
$$</p>
<p>But we only know the neighbors&rsquo; values if we know the graph.
What we do is:</p>
<h2 id="parallel-lasso-sparse-logistic-regressions">Parallel Lasso (sparse logistic regressions)</h2>
<ol>
<li>Perform $L_1$ regularized logistic regression of each node $Z_i$ on $Z_{-i}={Z_j, j\neq i}$ to estimate neighbors $\hat{N}(i)$.</li>
<li>Two versions:</li>
</ol>
<ul>
<li>create an edge $(i,j)$ if $j\in \hat{N}(i)$ <code>and</code> $i\in \hat{N}(j)$ (Smaller graph).</li>
<li>create an edge $(i,j)$ if $j\in \hat{N}(i)$ <code>or</code> $i\in \hat{N}(j)$ (Bigger graph).</li>
</ul>
<p>In theory, these methods work the same, and you can use either.
There is no graphical lasso because there is no closed form for the Ising model.
This is a different way of looking at the relationship between random variables than clustering.</p>
<h2 id="scaling-behavior-performance-with-data-size">Scaling behavior: Performance with data size</h2>
<p>How many data points do you need to estimate the graph correctly? Assume the model is correct, either Gaussian or Ising model.
Maximum degree $d$ of the $p$ variables. Sample size $n$ must satisfy:</p>
<ol>
<li>Ising model: $n\geq d^3 \log p$</li>
<li>Graphical lasso: $n\geq d^2 \log p$</li>
<li>Parallel lasso: $n\geq d \log p$</li>
<li>Lower bound: $n\geq d \log p$</li>
</ol>
<p>So why should we use a graphical lasso when a parallel Lasso needs less data?
<strong>Answer:</strong> Each method makes different incoherence assumptions: Correlations between unrelated variables are not too significant.</p>
<p>Number of edges $p^d$ and the number of bits to write down is $\log p^d = d \log p$</p>
<h2 id="graph-laplacian">Graph Laplacian</h2>
<p>Then, the graph Laplacian $L$ is the square $n \times n $ matrix defined as $L=D-A$.
The graph Laplacian gets its name from being the discrete analog of the Laplacian operator from calculus.</p>
<p><img src="../../images/sds-365/graph_estimation_examples_3.png" alt="Example"></p>
<h2 id="polynomials-of-the-laplacian">Polynomials of the Laplacian</h2>
<p>$$
P_w(L) \in \mathbb{R}^{n\times n} = w_0 I_n + w_1 L + w_2 L^2+ .. + w_d L^d = \sum_{i=0}^d w_i L^i
$$</p>
<p>Each polynomial of this form can alternately be represented by its vector of coefficients: $w=[w_0,..,w_d]$.
If $\text{dist}(u,v)&gt; I $, then the $(u,v)$ entry of $L^i$ is zero, which is analogous to a CNN filter (kernel).
The weights $w_i$ play the role of filter coefficients.
Degree $d$ of polynomial plays the role of the size of the kernel.</p>
<h2 id="the-laplacian-is-a-mercer-kernel">The Laplacian is a Mercer kernel</h2>
<ol>
<li>It&rsquo;s symmetric: $L_{uv} = L_{vu}$.</li>
<li>It&rsquo;s Positive-definite:
$$
f^TLf = \sum_{(u,v) \in E} (f_u-f_v)^2 \geq 0
$$</li>
</ol>
<h2 id="whence-equivariance">Whence equivariance</h2>
<p>A transformation $f: \mathbb{R}^n \rightarrow \mathbb{R}^n $ is equivariant if:
$$
f(Px) = Pf(x)
$$</p>
<p>for any permutation matrix $P$, where $PP^T=I$.
The transformed data and Laplacian are:
$$
\begin{aligned}
x\rightarrow Px \\
L \rightarrow PLP^T \\
L^i \rightarrow PL^iP^T
\end{aligned}
$$</p>
<p>The transformed polynomial kernels are:
$$
\begin{aligned}
f(Px) = &amp; \sum_{i=0}^d w_i (PL^iP^T) Px \\
=&amp; \sum_{i=0}^d w_i PL^ix \\
=&amp; P\sum_{i=0}^d w_i L^ix \\
=&amp; Pf(x) \\
\end{aligned}
$$</p>
<p>We covered this post in the intermediate machine learning SDS 365/565, Yale University, <a href="https://en.wikipedia.org/wiki/John_D._Lafferty">John Lafferty</a>, where I was TF.</p>




  </div>












</div>
  </main>

  <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>
magnify("myimage", 3);

</script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/js/all.min.js"
          integrity="sha256-MAgcygDRahs+F/Nk5Vz387whB4kSK9NXlDN3w58LLq0="
          crossorigin="anonymous"></script>


  <script src="../../js/jquery.min.js"></script>
  <script src="../../js/soho.js"></script>

  <script src="https://dadashkarimi.github.io/js/blog.js"></script>


</body>
</html>
